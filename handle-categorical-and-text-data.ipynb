{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Handle text features with many categories (50+) in Python","metadata":{}},{"cell_type":"code","source":"import pandas as ps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. **Label Encoding** \nFor ordinal or categorical features with a logical order:\nThis converts each unique category into an integer. \nHowever, this may not work well for non-ordinal features \nas it can introduce unintended relationships between categories.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \nencoder = LabelEncoder() \ndf['encoded_feature'] = encoder.fit_transform(df['text_feature'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. **One-Hot Encoding** \nFor non-ordinal categorical features with many unique values:\nThis creates binary columns for each unique category, but with 50+ categories, \nthis can result in high-dimensional data, which may not be ideal.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder \nencoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') \nencoded_feature = encoder.fit_transform(df[['text_feature']]) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. **Frequency/Count Encoding** \nYou can replace each category with its frequency in the dataset:\nThis helps preserve the information while keeping the feature one-dimensional.","metadata":{}},{"cell_type":"code","source":"df['freq_encoded_feature'] = df['text_feature'].map(df['text_feature'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. **Target/Mean Encoding** \nFor supervised learning, you can encode categories based on the mean of the target variable: \nThis method introduces information from the target, \nmaking it more powerful but at risk of data leakage.","metadata":{}},{"cell_type":"code","source":"mean_encoded_feature = df.groupby('text_feature')['target'].mean() \ndf['mean_encoded_feature'] = df['text_feature'].map(mean_encoded_feature) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. **Embedding Encoding** If you have large categorical features, \nyou can use embedding techniques, commonly seen with deep learning models like neural networks:\nThis approach is suitable for high-cardinality categorical features and can capture relationships between categories in a compact form.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf \nmodel = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=num_categories, output_dim=embedding_dim) ]) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. **Hashing Encoding** \nA dimensionality-reduction technique that maps categories into a fixed number of hash buckets: \nThis method works well when you need to reduce the dimensionality but maintain uniqueness. ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction import FeatureHasher \nhasher = FeatureHasher(n_features=10, input_type='string') \nhashed_feature = hasher.transform(df[['text_feature']].astype(str)) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}